{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Python clinic day 2: Corpus processing\n",
    "\n",
    "Na-Rae Han ([naraehan@pitt.edu](mailto:naraehan@pitt.edu)), 2017-07-13, [Pittsburgh NEH Institute “Make Your Edition”](https://github.com/Pittsburgh-NEH-Institute/Institute-Materials-2017) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "### Data\n",
    "\n",
    "- This tutorial is found in https://github.com/Pittsburgh-NEH-Institute/Institute-Materials-2017/tree/master/schedule/week_1\n",
    "- Download and unzip the “C-Span Inaugural Address Corpus”, available on NLTK’s corpora page: http://www.nltk.org/nltk_data/\n",
    "- Place the unzipped `inaugural` folder **on your desktop** \n",
    "\n",
    "### Jupyter tips\n",
    "\n",
    "- `Shift+ENTER` to run cell, go to next cell\n",
    "- `Alt+ENTER` to run cell, create a new cell below\n",
    "\n",
    "More on https://www.cheatography.com/weidadeyue/cheat-sheets/jupyter-notebook/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review \n",
    "- Let’s review [what we learned yesterday](Python Clinic Day 1.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a single text file, continued\n",
    "### Reading in a text file\n",
    "* Start by opening up the 1789 Washington address, using `open(filename).read()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile = 'C:/Users/narae/Desktop/inaugural/1789-Washington.txt'  # Use your own userid; Mac users should omit C:\n",
    "wtxt = open(myfile).read()\n",
    "print(wtxt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text, compile frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk    # Don't forget to import nltk\n",
    "%pprint    # Turn off/on pretty printing (prints too many lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtokens = nltk.word_tokenize(wtxt)\n",
    "len(wtokens)     # Number of words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a dictionary of frequency count\n",
    "wfreq = nltk.FreqDist(wtokens)\n",
    "wfreq['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(wfreq)      # Number of unique words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wfreq.most_common(40)     # 40 most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average sentence length, frequency of long words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentcount = wfreq['.'] + wfreq['?'] + wfreq['!']  # Assuming every sentence ends with ., ! or \n",
    "print(sentcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokens include symbols and punctuation. First 50 tokens:\n",
    "wtokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtokens_nosym = [t for t in wtokens if t.isalnum()]    # alpha-numeric tokens only\n",
    "len(wtokens_nosym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try \"n't\", \"20th\", \".\"\n",
    "\"n't\".isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First 50 tokens, alpha-numeric tokens only: \n",
    "wtokens_nosym[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(wtokens_nosym)/sentcount     # Average sentence length in number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[w for w in wfreq if len(w) >= 13]       # all 13+ character words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "long = [w for w in wfreq if len(w) >= 13] \n",
    "for w in long :\n",
    "    print(w, len(w), wfreq[w])               # long words tend to be less frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a  corpus\n",
    "\n",
    "- NLTK can read in an entire corpus from a directory (the “root” directory).\n",
    "- As it reads in a corpus, it applies word tokenization (`.words()`) and sentence tokenization (`.sents()`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'C:/Users/narae/Desktop/inaugural'  # Use your own userid; Mac users should omit C:\n",
    "inaug = PlaintextCorpusReader(corpus_root, '.*txt')  # all files ending in 'txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# .txt file names as file IDs\n",
    "inaug.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK automatically tokenizes the corpus. First 50 words: \n",
    "print(inaug.words()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also specify individual file ID. First 50 words from Obama 2009:\n",
    "print(inaug.words('2009-Obama.txt')[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK automatically segments sentences too, which are accessed through .sents()\n",
    "print(inaug.sents('2009-Obama.txt')[0])   # first sentence\n",
    "print(inaug.sents('2009-Obama.txt')[1])   # 2nd sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How long are these speeches in terms of word and sentence count?\n",
    "print('Washington 1789:', len(inaug.words('1789-Washington.txt')), len(inaug.sents('1789-Washington.txt')))\n",
    "print('Obama 2009:', len(inaug.words('2009-Obama.txt')), len(inaug.sents('2009-Obama.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for-loop through file IDs and print out word count. \n",
    "for f in inaug.fileids():\n",
    "    print(len(inaug.words(f)), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouble shooting \n",
    "\n",
    "- Unfortunately, 2005 Bush file produces a Unicode encoding error. \n",
    "- Let's make a new text file from [http://www.presidency.ucsb.edu/inaugurals.php](http://www.presidency.ucsb.edu/inaugurals.php)\n",
    "- Copy text and paste in Notepad (Windows) or BBEdit (Mac). Windows users: make sure to choose **UTF-8** encoding and not ANSI. \n",
    "- The text files are locked; We will need to save, halt and then re-start the Python notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corpus size in number of words\n",
    "print(len(inaug.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building word frequency distribution for the entire corpus\n",
    "inaug_freq = nltk.FreqDist(inaug.words())\n",
    "inaug_freq.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "Take a Python course! There are many online courses available on [Coursera](http://www.coursera.org), [EdX](https://www.edx.org/), [udemy](https://www.udemy.com/courses/), [DataCamp](https://www.datacamp.com/courses), and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
