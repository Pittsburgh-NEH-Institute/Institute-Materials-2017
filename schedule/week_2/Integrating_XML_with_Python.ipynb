{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating XML with Python\n",
    "\n",
    "[NLTK](http://www.nltk.org/), the Python Natural Languge ToolKit package, is designed to work with plain text input, but sometimes your input is in XML. There are two principal paths to reconciliation: either use an XML environment that supports NLP (natural language processing) or let Python (which supports NLP through NLTK) manage the XML. The first approach, sticking to an XML environment, is illustrated in Week 3 of the Institute in the context of the [eXist XML database](http://exist-db.org), which integrates the [Stanford Core NLP tools](https://stanfordnlp.github.io/CoreNLP/). Here we illustrate the second approach, letting Python manage the XML.\n",
    "\n",
    "## Before you make a mistake\n",
    "\n",
    "It’s natural to think of *parsing* (reading, interpreting, and processing) XML with regular expressions, but it’s also Wrong for at least two sets of reasons:\n",
    "\n",
    "1. Regular expressions operate over strings, and there are string differences in XML that are not informationally different. For example, the order of attributes on an element, whether the attributes are single- or double-quoted, whether a Unicode character is represented by a raw character or a numerical character reference, and many other details represent string differences that are not informational differences. The same is true of the extent and type of white space *in some environments but not others*. And the same is true when you have to recognize whether a right angle bracket or a single or double quotation mark is part of content or part of markup. XML-aware processing knows what’s informational and what isn’t, as well as what’s content and what’s markup. You don’t want to reinvent *those* wheels.\n",
    "\n",
    "1. Parsing XML is a recursive operation. For example, if you have two elements of the same type nested inside each other, as in\n",
    "```xml\n",
    "<emphasis><emphasis>a very emphatic thought</emphasis></emphasis>\n",
    "```\n",
    "parsing has to match up the correctly paired start and end tags. XML-aware processing knows where it is in the tree. That’s another wheel you don’t want to reinvent.\n",
    "\n",
    "It’s also natural to think of *writing* XML by constructing a string, such as concatenating angle brackets and text and other bits and pieces. This is a Bad Idea because some decisions are context sensitive, and keeping track of the context is challenging. For example, attribute values can be quoted with single or double quotation marks, but if the value contains a single or double quotation mark, that can influence the choice, and there are situations where you may need to represent the quotation marks in attribute values with `&quot;` or `&apos;` character entities instead of as raw characters. A library that knows how to write XML will keep track of that for you.\n",
    "\n",
    "## Wrangling XML in Python\n",
    "\n",
    "The Python Standard Library provides several [tools for parsing and creating XML](https://docs.python.org/3/library/markup.html), and there are also third-party packages. In this tutorial we use two parts of the Standard Library: [`pulldom`](https://docs.python.org/3/library/xml.dom.pulldom.html) for parsing XML input and [`minidom`](https://docs.python.org/3/library/xml.dom.minidom.html) for constructing XML output. You can read more about these modules by clicking on the preceding links to the Standard Library documentation, and also in the [Structured text: XML](http://etutorials.org/Programming/Python+tutorial/Part+IV+Network+and+Web+Programming/Chapter+23.+Structured+Text+XML/) chapter of the eTutorials.org [Python tutorial](http://etutorials.org/Programming/Python+tutorial/).\n",
    "\n",
    "To illustrate how to read and write XML with Python we’ll read in a small input XML document, tag each word as a `<word>` element, and add part of speech (POS) and lemma (dictionary form) information as `@pos` and `@lemma` attributes of the `<word>` elements. We’ll use `pulldom` to read, parse, and process the input document, NLTK to determine the part of speech and the lemma, and `minidom` to create the output. \n",
    "\n",
    "## Input XML\n",
    "\n",
    "Create the following small XML document in a work directory and save with a filename like `test.xml`:\n",
    "\n",
    "```xml\n",
    "<root>\n",
    "    <p speaker=\"hamlet\">Hamlet is a prince of Denmark.</p>\n",
    "    <p speaker='ophelia'>Things end badly for Ophelia.</p>\n",
    "    <p speaker=\"nobody\">Julius Caesar does not appear in this play.</p>\n",
    "</root>\n",
    "```\n",
    "\n",
    "## Desired output XML\n",
    "\n",
    "The desired output is:\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" ?>\n",
    "<root>\n",
    "\t<p speaker=\"hamlet\">\n",
    "\t\t<word lemma=\"hamlet\" pos=\"NNP\">Hamlet</word>\n",
    "\t\t<word lemma=\"be\" pos=\"VBZ\">is</word>\n",
    "\t\t<word lemma=\"a\" pos=\"DT\">a</word>\n",
    "\t\t<word lemma=\"prince\" pos=\"NN\">prince</word>\n",
    "\t\t<word lemma=\"of\" pos=\"IN\">of</word>\n",
    "\t\t<word lemma=\"denmark\" pos=\"NNP\">Denmark</word>\n",
    "\t\t<word lemma=\".\" pos=\".\">.</word>\n",
    "\t</p>\n",
    "\t<p speaker=\"ophelia\">\n",
    "\t\t<word lemma=\"thing\" pos=\"NNS\">Things</word>\n",
    "\t\t<word lemma=\"end\" pos=\"VBP\">end</word>\n",
    "\t\t<word lemma=\"badly\" pos=\"RB\">badly</word>\n",
    "\t\t<word lemma=\"for\" pos=\"IN\">for</word>\n",
    "\t\t<word lemma=\"ophelia\" pos=\"NNP\">Ophelia</word>\n",
    "\t\t<word lemma=\".\" pos=\".\">.</word>\n",
    "\t</p>\n",
    "\t<p speaker=\"nobody\">\n",
    "\t\t<word lemma=\"julius\" pos=\"NNP\">Julius</word>\n",
    "\t\t<word lemma=\"caesar\" pos=\"NNP\">Caesar</word>\n",
    "\t\t<word lemma=\"do\" pos=\"VBZ\">does</word>\n",
    "\t\t<word lemma=\"not\" pos=\"RB\">not</word>\n",
    "\t\t<word lemma=\"appear\" pos=\"VB\">appear</word>\n",
    "\t\t<word lemma=\"in\" pos=\"IN\">in</word>\n",
    "\t\t<word lemma=\"this\" pos=\"DT\">this</word>\n",
    "\t\t<word lemma=\"play\" pos=\"NN\">play</word>\n",
    "\t\t<word lemma=\".\" pos=\".\">.</word>\n",
    "\t</p>\n",
    "</root>\n",
    "```\n",
    "\n",
    "## The python code\n",
    "\n",
    "## Before you run the code\n",
    "\n",
    "NLTK is installed by default with Anaconda python, but the word tokenizer isn’t. To install the tokenizer, **uncomment the second line below** and run (if you’ve already installed the tokenizer, run the cell without uncommenting the second line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A separate window will open. Select the `Models` tab on the top, then `averaged_perceptron_tagger` and `punkt`, and then press the Download button. Then select the `Corpora` tab on the top, then `wordnet`, and then Download. You only have to download these once on each machine you use; the download process will install them for future use.\n",
    "\n",
    "## The annotation code\n",
    "\n",
    "Here is the entire Python script that creates the output (we describe how the pieces work below). If you have saved the sample input as `test.xml` in the same directory as the location of this notebook, you can run the transformation in the notebook now, and the output should be displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<root>\n",
      "\t<l speaker=\"hamlet\">\n",
      "\t\t<word lemma=\"hamlet\" pos=\"NNP\">Hamlet</word>\n",
      "\t\t<word lemma=\"be\" pos=\"VBZ\">is</word>\n",
      "\t\t<word lemma=\"a\" pos=\"DT\">a</word>\n",
      "\t\t<word lemma=\"princess\" pos=\"NN\">princess</word>\n",
      "\t\t<word lemma=\"of\" pos=\"IN\">of</word>\n",
      "\t\t<word lemma=\"denmark\" pos=\"NNP\">Denmark</word>\n",
      "\t\t<word lemma=\".\" pos=\".\">.</word>\n",
      "\t</l>\n",
      "\t<l speaker=\"ophelia\">\n",
      "\t\t<word lemma=\"thing\" pos=\"NNS\">Things</word>\n",
      "\t\t<word lemma=\"end\" pos=\"VBP\">end</word>\n",
      "\t\t<choice>\n",
      "\t\t\t<sic>\n",
      "\t\t\t\t<word lemma=\"badly\" pos=\"RB\">badly</word>\n",
      "\t\t\t</sic>\n",
      "\t\t\t<corr>\n",
      "\t\t\t\t<word lemma=\"horrible\" pos=\"JJ\">horrible</word>\n",
      "\t\t\t</corr>\n",
      "\t\t</choice>\n",
      "\t\t<word lemma=\"for\" pos=\"IN\">for</word>\n",
      "\t\t<word lemma=\"ophelia\" pos=\"NNP\">Ophelia</word>\n",
      "\t\t<word lemma=\"'s\" pos=\"POS\">'s</word>\n",
      "\t\t<word lemma=\"mother\" pos=\"NN\">mother</word>\n",
      "\t\t<word lemma=\".\" pos=\".\">.</word>\n",
      "\t</l>\n",
      "\t<l speaker=\"nobody\">\n",
      "\t\t<word lemma=\"julius\" pos=\"NNP\">Julius</word>\n",
      "\t\t<word lemma=\"caesar-ion\" pos=\"NN\">Caesar-ion</word>\n",
      "\t\t<l>\n",
      "\t\t\t<word lemma=\"dude\" pos=\"NN\">dude</word>\n",
      "\t\t</l>\n",
      "\t\t<word lemma=\"do\" pos=\"VBZ\">does</word>\n",
      "\t\t<word lemma=\"not\" pos=\"RB\">not</word>\n",
      "\t\t<word lemma=\"appear\" pos=\"VB\">appear</word>\n",
      "\t\t<word lemma=\"in\" pos=\"IN\">in</word>\n",
      "\t\t<word lemma=\"this\" pos=\"DT\">this</word>\n",
      "\t\t<word lemma=\"play\" pos=\"NN\">play</word>\n",
      "\t\t<word lemma=\".\" pos=\".\">.</word>\n",
      "\t</l>\n",
      "</root>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"Tag words and add POS and lemma information in XML document.\"\"\"\n",
    "\n",
    "from xml.dom.minidom import Document, Element\n",
    "from xml.dom import pulldom\n",
    "import nltk\n",
    "\n",
    "\n",
    "def create_word_element(d: Document, text: str, pos: str) -> Element:\n",
    "    \"\"\"Create <word> element with POS and lemma attributes.\"\"\"\n",
    "    word = d.createElement(\"word\")\n",
    "    word.setAttribute(\"pos\", pos)\n",
    "    word.setAttribute(\"lemma\", lemmatize(text, pos))\n",
    "    t = d.createTextNode(text)\n",
    "    word.appendChild(t)\n",
    "    return word\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag: str) -> str:\n",
    "    \"\"\"Replace treebank POS tags with wordnet ones; default POS is noun.\"\"\"\n",
    "    pos_tags = {'J': nltk.corpus.reader.wordnet.ADJ, 'V': nltk.corpus.reader.wordnet.VERB,\n",
    "                'R': nltk.corpus.reader.wordnet.ADV}\n",
    "    return pos_tags.get(treebank_tag[0], nltk.corpus.reader.wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize(text: str, pos: str) -> str:\n",
    "    \"\"\"Identify lemma for current word.\"\"\"\n",
    "    return nltk.stem.WordNetLemmatizer().lemmatize(text.lower(), get_wordnet_pos(pos))\n",
    "\n",
    "\n",
    "def extract(input_xml) -> Document:\n",
    "    \"\"\"Process entire input XML document, firing on events.\"\"\"\n",
    "    # Initialize output as XML document, point to most recent open node\n",
    "    d = Document()\n",
    "    current = d\n",
    "    # Start pulling; it continues automatically\n",
    "    doc = pulldom.parse(input_xml)\n",
    "    for event, node in doc:\n",
    "        if event == pulldom.START_ELEMENT:\n",
    "            current.appendChild(node)\n",
    "            current = node\n",
    "        elif event == pulldom.END_ELEMENT:\n",
    "            current = node.parentNode\n",
    "        elif event == pulldom.CHARACTERS:\n",
    "            # tokenize, pos-tag, create <word> as child of parent\n",
    "            words = nltk.word_tokenize(node.toxml())\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            for (text, pos) in tagged_words:\n",
    "                word = create_word_element(d, text, pos)\n",
    "                current.appendChild(word)\n",
    "    return d\n",
    "\n",
    "\n",
    "with open('test-1.xml', 'r') as test_in:\n",
    "    results = extract(test_in)\n",
    "    print(results.toprettyxml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "We’ve divided the program into sections below, with explanations after each section.\n",
    "\n",
    "### Shebang and docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tag words and add POS and lemma information in XML document.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"Tag words and add POS and lemma information in XML document.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Python program begins with a *shebang* and a [*docstring*](https://www.python.org/dev/peps/pep-0257/). The shebang makes it easier to run the program from the command line, and the *docstring* documents what the program does, The shebang must be the very first line in a program. For now, think of the shebang as a magic incantation that should be copied and pasted verbatim; we explain below what it means. The docstring should be a single line framed by triple  quotation marks, and it should describe concisely what the program does. When you execute the docstring by itself, as we do above, it echoes itself to the screen; when you run the program, though, it remains silent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom.minidom import Document\n",
    "from xml.dom import pulldom\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the ability to create a new XML document, which we’ll use to create our output, from `minidom`, and we import `pulldom` to parse the input document. We import `nltk` because we’ll use it to determine the part of speech and the lemma for each word.\n",
    "\n",
    "### Adding a `<word>` element to the output tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_element(d: Document, text: str, pos: str) -> Element:\n",
    "    \"\"\"Create <word> element with POS and lemma attributes.\"\"\"\n",
    "    word = d.createElement(\"word\")\n",
    "    word.setAttribute(\"pos\", pos)\n",
    "    word.setAttribute(\"lemma\", lemmatize(text, pos))\n",
    "    t = d.createTextNode(text)\n",
    "    word.appendChild(t)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we tokenize the text into words below, we pass each word and its part of speech into the `create_word_element()` function. The function creates a new `<word>` element, adds the part of speech tag as an attribute, and then uses our `lemmatize()` function to determine the lemma and add that as an attribute, as well. It then creates a `text()` node, sets its value as the text of the word, and makes the `text()` node a child of the new `<word>` element. Finally, we return the `<word>` element to the calling routine, which inserts it into the output XML tree in the right place.\n",
    "\n",
    "### Converting treebank part of speech identifiers to Wordnet ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag: str) -> str:\n",
    "    \"\"\"Replace treebank POS tags with wordnet ones; default POS is noun.\"\"\"\n",
    "    pos_tags = {'J': nltk.corpus.reader.wordnet.ADJ, 'V': nltk.corpus.reader.wordnet.VERB,\n",
    "                'R': nltk.corpus.reader.wordnet.ADV}\n",
    "    return pos_tags.get(treebank_tag[0], nltk.corpus.reader.wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function called `get_wordnet_pos()`, which we’ll use later. This function is defined as taking one argument, called `treebank_tag`, which is a string, and it returns a value that is also a string. The reason we need to do this is that the NLTK part of speech tagger uses one set of part of speech identifiers, but Wordnet, the NLTK component that performs lemmatization, uses a different one. Since we do the part of speech tagging first, we use this function to convert that value to one that Wordnet will understand before we perform lemmatization. There are many treebank part of speech tags but only four Wordnet ones, for nouns, verbs, adjectives, and adverbs, and everything else is treated as a noun. Our function returns the correct value for the four defined parts of speech and defaults to the value for nouns otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(text: str, pos: str) -> str:\n",
    "    return nltk.stem.WordNetLemmatizer().lemmatize(text.lower(), get_wordnet_pos(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function called `lemmatize()` that takes two pieces of input, both of which are strings, and returns a string. The parameter `text` is the word to be lemmatized and the parameter `pos` is the part of speech in treebank form. We call the NLTK function to identify the lemma with `nltk.stem.WordNetLemmatizer().lemmatize()` with two arguments. The lemmatizer expects words to be lower case, so we convert the `text` to lower case with the `lower()` string method. And it requires a Wordnet part of speech, and not a treebank one, so we use our `get_wordnet_pos()` function to perform the conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(input_xml) -> Document:\n",
    "    \"\"\"Process entire input XML document, firing on events.\"\"\"\n",
    "    # Initialize output as XML document, point to most recent open node\n",
    "    d = Document()\n",
    "    current = d\n",
    "    # Start pulling; it continues automatically\n",
    "    doc = pulldom.parse(input_xml)\n",
    "    for event, node in doc:\n",
    "        if event == pulldom.START_ELEMENT:\n",
    "            current.appendChild(node)\n",
    "            current = node\n",
    "        elif event == pulldom.END_ELEMENT:\n",
    "            current = node.parentNode\n",
    "        elif event == pulldom.CHARACTERS:\n",
    "            # tokenize, pos-tag, create <word> as child of parent\n",
    "            words = nltk.word_tokenize(node.toxml())\n",
    "            tagged_words = nltk.pos_tag(words)\n",
    "            for (text, pos) in tagged_words:\n",
    "                word = create_word_element(d, text, pos)\n",
    "                current.appendChild(word)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer below to line numbers, and if you’re reading this on line, you won’t those numbers. You can make them appear by running this notebook in a Jupyter session, clicking in the cell above, hitting the `Esc` key, to switch into command mode, and then typing `l` (the lowercase letter `L`), to toggle line numbering.\n",
    "\n",
    "Our `extract()` function does all the work, calling on the functions we defined earlier as needed. Here’s how it works (with line numbers):\n",
    "\n",
    "* **1:** `extract()` is a function that gets called with one argument, which we assign to a parameter we’ve called `input_xml`.\n",
    "* **4:** Near the top of the full program we’ve already used `from xml.dom.minidom import Document, Element` to make the `Document` class (and the `Element` class) available to our program. Here we use it to create a new XML document, which we assign as the value of a new variable `d`. We’ll use this to build our output document.\n",
    "* **5:** The variable `current` points to the node that will be the parent of any new elements. The document node is the root of the entire document, so it’s the initial value of the `current` variable.\n",
    "* **y:** `pulldom` is a streaming parser, which means that once we start processing elements in the XML input tree, the parser keeps going until it has visited every node of the tree. We start that process with `pulldom.parse()`, telling it to parse the document we passed to it as the value of the `input_xml` parameter.\n",
    "* **8:** Parsing generates events like the start or end of an element or the presence of character data. There are other possible events, but these are the only ones we need to handle for our transformation. Each event provides a tuple that consists of two values, the name of the event (e.g., `START_ELEMENT`) and the value (e.g., an object of type `node`). We test the event type and process different types of events differently.\n",
    "* **9–11:** When we start a new element, we make it a child of the node identified by our `current` variable. This ensures that the output tree that we’re building will reproduce the structure of the input tree, and it also ensures that we create new `<word>` elements in the correct place. When we start an element, it’s the parent of any nodes we encounter until we find the corresponding `END_ELEMENT` event, so we make it a child of whatever node is current at the moment and then set the `current` variable to point to the node we just created. This means that, for example, when we encounter the first child of the root element of the input XML, we’ll make that a child of the root element of the output XML that we’re constructing.\n",
    "* **12–13:** When we encounter an `END_ELEMENT` event, that element can’t have any more children, so we set the `current` variable to point to its parent.\n",
    "* **14–20:** We’ll illustrate how the individual lines work below, but here’s a summary with everything in one place. When we encounter `CHARACTERS` while parsing, the value of the node is an XML `text()` node, and not a string. We convert it to a plain text string with the `toxml()` method, let NLTK break it into words with `nltk.word_tokenize()`, and assign the pieces to an array called `words` (line **16**). Next, the `nltk.pos_tag()` function takes an array of words as its input (our `words` variable) and returns an array of tuples, that is, pairs of strings where the first is the original input word and the second is the part of speech according to treebank notation (**17**). It assigns this new array as the value of the `tagged_words` variable. We want to create a new `<word>` element in the output for each word, so we loop over that list of tuples (**18**). For each word, we call our `create_word_element()` function, which we defined earlier, and set the value of the variable `word` equal to the new `<word>` element (**19**). Finally, we make the new word a child of the current element, the one that was its parent in the input (**20**). There are other types of parse events, but we don’t need to do anything with them in this example, so we don’t write any code to process them.\n",
    "\n",
    "## Remind me about those NLTK functions again\n",
    "\n",
    "### `nltk.word_tokenize()`\n",
    "\n",
    "`nltk.word_tokenize()` splits a text into words. It’s smarter than just splitting on white space; it treats punctuation as a word, and it knows about common English contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'realize',\n",
       " 'that',\n",
       " 'we',\n",
       " 'could',\n",
       " 'split',\n",
       " 'contractions',\n",
       " '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"We didn't realize that we could split contractions!\"\n",
    "nltk.word_tokenize(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nltk.pos_tag()`\n",
    "\n",
    "`nltk.pos_tag()` takes a list of words (not a sentence) as its input. That means that we need to tokenize the sentence before tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('did', 'VBD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('realize', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('could', 'MD'),\n",
       " ('split', 'VB'),\n",
       " ('contractions', 'NNS'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"We didn't realize that we could split contractions!\"\n",
    "words = nltk.word_tokenize(sample)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look up the part of speech tags at <https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html>.\n",
    "\n",
    "### `nltk.stem.WordNetLemmatizer().lemmatize()`\n",
    "\n",
    "The Wordnet lemmatizer tries to lemmatize (find the dictionary form) of a word with or without part of speech information, but without the part of speech, it guesses that everything is a noun. Remember that Wordnet knows about only nouns, verbs, adjectives, and adverbs, and that the part of speech tags are different in Wordnet than in the treebank system. Oh, and it assumes lower-case input, so if you give it a capitalized word, it won’t recognize it as an inflected form of something else, and will therefore return it unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing: thing', 'things: thing', 'Things: Things']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['thing', 'things', 'Things']\n",
    "[(word + \": \" + nltk.stem.WordNetLemmatizer().lemmatize(word)) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the lemmatizer recognizes that “thing” is the lemma for “things”, but it fails to lemmatize “Things” correctly because of the upper case letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['building: building', 'building: build']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [('building','n'), ('building','v')]\n",
    "[(word + \": \" + nltk.stem.WordNetLemmatizer().lemmatize(word, pos)) for (word, pos) in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we supplied part of speech information, and the lemmatizer correctly treats “building” differently as a noun than as a verb. If we don’t specify a part of speech, it assumes everything is a noun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['building: building']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['building']\n",
    "[(word + \": \" + nltk.stem.WordNetLemmatizer().lemmatize(word)) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<root>\n",
      "\t<p speaker=\"hamlet\">\n",
      "\t\t<word lemma=\"hamlet\" pos=\"NNP\">Hamlet</word>\n",
      "\t\t<word lemma=\"be\" pos=\"VBZ\">is</word>\n",
      "\t\t<word lemma=\"a\" pos=\"DT\">a</word>\n",
      "\t\t<word lemma=\"prince\" pos=\"NN\">prince</word>\n",
      "\t\t<word lemma=\"of\" pos=\"IN\">of</word>\n",
      "\t\t<word lemma=\"denmark\" pos=\"NNP\">Denmark</word>\n",
      "\t\t<word lemma=\".\" pos=\".\">.</word>\n",
      "\t</p>\n",
      "\t<p speaker=\"ophelia\">\n",
      "\t\t<word lemma=\"thing\" pos=\"NNS\">Things</word>\n",
      "\t\t<word lemma=\"end\" pos=\"VBP\">end</word>\n",
      "\t\t<word lemma=\"badly\" pos=\"RB\">badly</word>\n",
      "\t\t<word lemma=\"for\" pos=\"IN\">for</word>\n",
      "\t\t<word lemma=\"ophelia\" pos=\"NNP\">Ophelia</word>\n",
      "\t\t<word lemma=\".\" pos=\".\">.</word>\n",
      "\t</p>\n",
      "\t<p speaker=\"nobody\">\n",
      "\t\t<word lemma=\"julius\" pos=\"NNP\">Julius</word>\n",
      "\t\t<word lemma=\"caesar\" pos=\"NNP\">Caesar</word>\n",
      "\t\t<word lemma=\"do\" pos=\"VBZ\">does</word>\n",
      "\t\t<word lemma=\"not\" pos=\"RB\">not</word>\n",
      "\t\t<word lemma=\"appear\" pos=\"VB\">appear</word>\n",
      "\t\t<word lemma=\"in\" pos=\"IN\">in</word>\n",
      "\t\t<word lemma=\"this\" pos=\"DT\">this</word>\n",
      "\t\t<word lemma=\"play\" pos=\"NN\">play</word>\n",
      "\t\t<word lemma=\".\" pos=\".\">.</word>\n",
      "\t</p>\n",
      "</root>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('test.xml', 'r') as test_in:\n",
    "    results = extract(test_in)\n",
    "    print(results.toprettyxml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could, alternatively, have opened a file handle to read the file from disk, read it, and saved the results with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = open('test.xml','r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn’t considered good practice, though, because it leaves the file handle (the way the program interacts with the file) open when it’s done, that is, even when it no longer needs it. Python eventually closes file handles, so no real harm is done, but there are situations where failing to close a file handle can have adverse consequences. For that reason, it’s good practice always to use the `with` construction to open files, since it ensures that they will be closed properly as soon as they are no longer being used. In this case, we open `test.xml` and assign it to a new variable called `test_in`. In the second argument to the `open()` command, the `r` opens the file for reading.\n",
    "\n",
    "We use the file as input to the `extract()` function we defined earlier, and when the function returns its results (the new XML document it has created), we assign those results to a new variable that we call `results`. Since that’s an XML document (a tree), we need to serialize it (convert it to a character stream) before we can output it by using the `print()` function. The `toprettyxml()` method of a `minidom` document serializes the tree and pretty-prints it, that is, indents it to make the hierarchy easier to read. You can, alternatively, serialize it without pretty-printing with the `toxml()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## In case you’re curious\n",
    "\n",
    "The following information isn’t needed for Institute activities, but if you expect to doing complex processing of XML in Python, here is a brief survey of the options.\n",
    "\n",
    "### XML support in Python\n",
    "\n",
    "If you’ve used XSLT (except for the new streaming facility of XSLT 3.0) to process XML before, you’ve been doing DOM-based (Document Object Model) processing, which parses the input, builds the entire tree in memory, and operates over it. DOM-based processing makes the entire tree available at all times, which is often what you want, but it isn’t the most efficient (in terms of speed and memory) approach, so if you don’t need the entire tree at once, you may prefer an alternative. Python has support for the DOM, which you can read about in the Standard Library reference in [`xml.dom` — The Document Object Model API](https://docs.python.org/3/library/xml.dom.html) and at [Parsing XML with DOM](http://etutorials.org/Programming/Python+tutorial/Part+IV+Network+and+Web+Programming/Chapter+23.+Structured+Text+XML/23.3+Parsing+XML+with+DOM/). For constructing XML, Python also provides [`xml.dom.minidom` — Minimal DOM implementation](https://docs.python.org/3/library/xml.dom.minidom.html), which “is intended to be simpler than the full DOM and also significantly smaller”. The most complete DOM resource in Python is [`xml.etree.ElementTree` — The ElementTree XML API](https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree), and the third-part [lxml](http://lxml.de/) package is further enhanced. See the note below about Installing `lxml` if you’d like to try working with it.\n",
    "\n",
    "The primary alternative to DOM parsing in the XML world is SAX (Simple API for XML; API = ‘application programming interface’), which is a streaming parser. Instead of building the entire tree in memory, a streaming parser acts on *events* like the start or end of an element, an attribute, character data, etc., and it processes each event and then moves onto the next one. In situations where you can do everything you need with an event right away, and don’t need to return to it later, SAX will be faster than DOM. Python has support for SAX processing, which you can read about in the Standard Library reference in [`xml.sax` — Support for SAX2 parsers](https://docs.python.org/3/library/xml.sax.html) or in [Parsing XML with SAX](http://etutorials.org/Programming/Python+tutorial/Part+IV+Network+and+Web+Programming/Chapter+23.+Structured+Text+XML/23.2+Parsing+XML+with+SAX/).\n",
    "\n",
    "`pulldom`, which we use in this tutorial, has been described as follows:\n",
    "\n",
    "> `pulldom` occupies an interesting middle ground between SAX and DOM, presenting the stream of parsing events as a Python iterator object so that you do not code callbacks, but rather loop over the events and examine each event to see if it’s of interest. When you do find an event of interest to your application, you can ask `pulldom` to build the DOM subtree rooted in that event’s node by calling method `expandNode`, and then work with that subtree as you would in `minidom`. Paul Prescod, `pulldom`’s author and XML and Python expert, describes the net result as “80% of the performance of SAX, 80% of the convenience of DOM.” ([Parsing XML with DOM](http://etutorials.org/Programming/Python+tutorial/Part+IV+Network+and+Web+Programming/Chapter+23.+Structured+Text+XML/23.3+Parsing+XML+with+DOM/))\n",
    "\n",
    "### Installing lxml\n",
    "\n",
    "Mac and Linux users can install `lxml` through [pypi](https://pypi.python.org/pypi/lxml/) by typing `pip install lxml`, but this installation requires developer tools that Windows users typically don’t have installed. Windows users can instead download prebuilt binaries from Christopher Gohlke’s [Unofficial Windows Binaries for Python Extension Packages](http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml) and install them by following the [Installing from wheels instructions](https://pip.pypa.io/en/latest/user_guide/#installing-from-wheels).\n",
    "\n",
    "### About the shebang\n",
    "\n",
    "You can run a Python program called something like `tag_words.py` from the command line by typing `python tag_words.py`. This starts the Python interpreter and tells it that the program it should interpret (and run) is a file called `tag_words.py` in the current directory. On a Unix or Mac system, though, you can also run it without specifying `python` first, that is, by making it an executable program. To do that:\n",
    "\n",
    "1. At the command line in the directory where the program file is located, type `chmod +x tag_words.py`. The `chmod` (change mode) command sets access rights for reading, writing, and execution, and the `+x` makes this file executable. That means that it no longer needs to be executed by calling Python separately; it knows how to execute itself. You need to do this only once; the file will remain executable, even if you edit it later.\n",
    "1. Since programs in many languages can be made executable, how does the system know what kind of program it is, that is, how does it know to execute this one as a *Python* program? That’s what the shebang does. The leading `#!` identifies the line as a shebang (only if it’s the very first line of the file), and you can read how it works at <https://stackoverflow.com/questions/43793040/how-does-usr-bin-env-work-in-a-linux-shebang-line>. \n",
    "1. Once you’ve configured the shebang and made the file executable, it’s a program you can run, but to run a program either you must tell the system where it’s located or it has to be in the execution path in your environment (you can examine that with `echo $PATH`). By default the current directory is not automatically in the path. You can either move the executable file into a directory that is in your path (a common choice is `/usr/local/bin`) or specify the path to it in the current directory by prepending `./`, e.g., `./tag_words.py`. The dot means ‘current directory’, and when you supply a path to an executable, your system can run it without having to look for it on the environment execution path.\n",
    "\n",
    "### Comments in `pulldom`\n",
    "\n",
    "`pulldom` is supposed to be able to respond to comment events, that is, to comments (delimited by `<!--` and `-->`) in the XML input. It doesn’t work when we try it. If comments are important to your processing, you’ll need to use a parser other than `pulldom`; in this case, we happen not to care."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
