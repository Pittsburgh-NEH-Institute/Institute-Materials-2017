{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding tokenization and normalization to collation\n",
    "\n",
    "## Starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-------+---------------------+------+------+\n",
      "| A | The | quick | brown | fox jumped over the | lazy | dog. |\n",
      "| B | The | -     | brown | fox jumped over the | -    | dog. |\n",
      "| C | The | bad   | -     | fox jumped over the | lazy | dog. |\n",
      "+---+-----+-------+-------+---------------------+------+------+\n"
     ]
    }
   ],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "collation.add_plain_witness( \"A\", \"The quick brown fox jumped over the lazy dog.\")\n",
    "collation.add_plain_witness( \"B\", \"The brown fox jumped over the dog.\" )\n",
    "collation.add_plain_witness( \"C\", \"The bad fox jumped over the lazy dog.\" )\n",
    "table = collate(collation)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the text of the witness from its inclusion in the collation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-------+---------------------+------+------+\n",
      "| A | The | quick | brown | fox jumped over the | lazy | dog. |\n",
      "| B | The | -     | brown | fox jumped over the | -    | dog. |\n",
      "| C | The | bad   | -     | fox jumped over the | lazy | dog. |\n",
      "+---+-----+-------+-------+---------------------+------+------+\n"
     ]
    }
   ],
   "source": [
    "collation = Collation()\n",
    "A_content = \"The quick brown fox jumped over the lazy dog.\"\n",
    "B_content = \"The brown fox jumped over the dog.\"\n",
    "C_content = \"The bad fox jumped over the lazy dog.\" \n",
    "collation.add_plain_witness( \"A\", A_content )\n",
    "collation.add_plain_witness( \"B\", B_content )\n",
    "collation.add_plain_witness( \"C\", C_content )\n",
    "table = collate(collation)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use functions to tokenize the witness text. Start with just one witness, and verify the result by outputting JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
      "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokens': [{'t': 'The '}, {'t': 'quick '}, {'t': 'brown '}, {'t': 'fox '}, {'t': 'jumped '}, {'t': 'over '}, {'t': 'the '}, {'t': 'lazy '}, {'t': 'dog.'}], 'id': 'A'}]\n",
      "\n",
      "\n",
      "{'witnesses': [{'tokens': [{'t': 'The '}, {'t': 'quick '}, {'t': 'brown '}, {'t': 'fox '}, {'t': 'jumped '}, {'t': 'over '}, {'t': 'the '}, {'t': 'lazy '}, {'t': 'dog.'}], 'id': 'A'}]}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input}\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"The quick brown fox jumped over the lazy dog.\"\n",
    "B_content = \"The brown fox jumped over the dog.\"\n",
    "C_content = \"The bad fox jumped over the lazy dog.\" \n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "print(witness_list)\n",
    "print(\"\\n\")\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "print(json_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add simple normalization. This won’t affect the collation output, but we can verify that it’s working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'witnesses': [{'tokens': [{'n': 'the ', 't': 'The '}, {'n': 'quick ', 't': 'quick '}, {'n': 'brown ', 't': 'brown '}, {'n': 'fox ', 't': 'fox '}, {'n': 'jumped ', 't': 'jumped '}, {'n': 'over ', 't': 'over '}, {'n': 'the ', 't': 'the '}, {'n': 'lazy ', 't': 'lazy '}, {'n': 'dog.', 't': 'dog.'}], 'id': 'A'}, {'tokens': [{'n': 'the ', 't': 'The '}, {'n': 'brown ', 't': 'brown '}, {'n': 'fox ', 't': 'fox '}, {'n': 'jumped ', 't': 'jumped '}, {'n': 'over ', 't': 'over '}, {'n': 'the ', 't': 'the '}, {'n': 'dog.', 't': 'dog.'}], 'id': 'B'}, {'tokens': [{'n': 'the ', 't': 'The '}, {'n': 'bad ', 't': 'bad '}, {'n': 'fox ', 't': 'fox '}, {'n': 'jumped ', 't': 'jumped '}, {'n': 'over ', 't': 'over '}, {'n': 'the ', 't': 'the '}, {'n': 'lazy ', 't': 'lazy '}, {'n': 'dog.', 't': 'dog.'}], 'id': 'C'}]}\n",
      "+---+-----+-------+-------+---------------------+------+------+\n",
      "| A | The | quick | brown | fox jumped over the | lazy | dog. |\n",
      "| B | The | -     | brown | fox jumped over the | -    | dog. |\n",
      "| C | The | bad   | -     | fox jumped over the | lazy | dog. |\n",
      "+---+-----+-------+-------+---------------------+------+------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize(input):\n",
    "    return input.lower()\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input, \"n\": normalize(input)}\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"The quick brown fox jumped over the lazy dog.\"\n",
    "B_content = \"The brown fox jumped over the dog.\"\n",
    "C_content = \"The bad fox jumped over the lazy dog.\" \n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "witness_list.append({\"id\": \"B\", \"tokens\": tokenize(B_content)})\n",
    "witness_list.append({\"id\": \"C\", \"tokens\": tokenize(C_content)})\n",
    "\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "print(json_input)\n",
    "table = collate(json_input)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the text to create a more complex example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---------+--------+\n",
      "| A | Look, a | gray | -       | koala! |\n",
      "| B | Look, a | big  | grey    | koala! |\n",
      "| C | Look, a | big  | wombat! | -      |\n",
      "+---+---------+------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize(input):\n",
    "    return input.lower()\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input, \"n\": normalize(input)}\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"Look, a gray koala!\"\n",
    "B_content = \"Look, a big grey koala!\"\n",
    "C_content = \"Look, a big wombat!\" \n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "witness_list.append({\"id\": \"B\", \"tokens\": tokenize(B_content)})\n",
    "witness_list.append({\"id\": \"C\", \"tokens\": tokenize(C_content)})\n",
    "\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "# print(json_input)\n",
    "table = collate(json_input)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhance normalization to recognize that all animals are alike. (This introduces possible complications, which can be addressed through further enhancements.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'witnesses': [{'tokens': [{'n': 'look ', 't': 'Look, '}, {'n': 'a ', 't': 'a '}, {'n': 'gray ', 't': 'gray '}, {'n': 'ANIMAL', 't': 'koala!'}], 'id': 'A'}, {'tokens': [{'n': 'look ', 't': 'Look, '}, {'n': 'a ', 't': 'a '}, {'n': 'big ', 't': 'big '}, {'n': 'grey ', 't': 'grey '}, {'n': 'ANIMAL', 't': 'koala!'}], 'id': 'B'}, {'tokens': [{'n': 'look ', 't': 'Look, '}, {'n': 'a ', 't': 'a '}, {'n': 'big ', 't': 'big '}, {'n': 'ANIMAL', 't': 'wombat!'}], 'id': 'C'}]}\n",
      "+---+---------+------+------+---------+\n",
      "| A | Look, a | gray | -    | koala!  |\n",
      "| B | Look, a | big  | grey | koala!  |\n",
      "| C | Look, a | big  | -    | wombat! |\n",
      "+---+---------+------+------+---------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def normalize(input):\n",
    "    # \"string.punctuation\" returns string of all punctuation marks Python knows\n",
    "    # the [] are a regex character class\n",
    "    # this subs all punctuation for an empty space\n",
    "    input = re.sub('[' + string.punctuation + ']','',input)\n",
    "    animals = ['koala', 'wombat']\n",
    "    # animals is a list of all animals\n",
    "    if input in animals:\n",
    "        return 'ANIMAL'\n",
    "    else:\n",
    "        return input.lower()\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input, \"n\": normalize(input)}\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"Look, a gray koala!\"\n",
    "B_content = \"Look, a big grey koala!\"\n",
    "C_content = \"Look, a big wombat!\" \n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "witness_list.append({\"id\": \"B\", \"tokens\": tokenize(B_content)})\n",
    "witness_list.append({\"id\": \"C\", \"tokens\": tokenize(C_content)})\n",
    "\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "print(json_input)\n",
    "table = collate(json_input)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The animals are now aligned, but the colors aren’t. We can address that through matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----+------+---------+\n",
      "| A | Look, | a | -   | gray | koala!  |\n",
      "| B | Look, | a | big | grey | koala!  |\n",
      "| C | Look, | a | big | -    | wombat! |\n",
      "+---+-------+---+-----+------+---------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def normalize(input):\n",
    "    input = re.sub('[' + string.punctuation + ']','',input)\n",
    "    animals = ['koala', 'wombat']\n",
    "    if input in animals:\n",
    "        return 'ANIMAL'\n",
    "    else:\n",
    "        return input.lower()\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input, \"n\": normalize(input)}\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"Look, a gray koala!\"\n",
    "B_content = \"Look, a big grey koala!\"\n",
    "C_content = \"Look, a big wombat!\" \n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "witness_list.append({\"id\": \"B\", \"tokens\": tokenize(B_content)})\n",
    "witness_list.append({\"id\": \"C\", \"tokens\": tokenize(C_content)})\n",
    "\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "# print(json_input)\n",
    "# CX's near_matching looks for the closest match\n",
    "table = collate(json_input, near_match=True, segmentation=False)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
