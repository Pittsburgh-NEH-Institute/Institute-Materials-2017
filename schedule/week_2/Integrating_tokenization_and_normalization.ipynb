{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating normalization with tokenization\n",
    "\n",
    "## Structure\n",
    "\n",
    "We define functions to tokenize and normalize. We tokenize the input and then normalize each token. This first example is here only to illustrate the general program logic. If you try to run it, it returns an error because we haven’t defined the functions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(input):\n",
    "    pass\n",
    "\n",
    "def normalize(input):\n",
    "    pass\n",
    "\n",
    "sample = \"Hello, Mom!\"\n",
    "tokens = tokenize(sample)\n",
    "# print(tokens)\n",
    "normalized = [normalize(token) for token in tokens]\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Tokenize on white space and normalize as lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(input): # tokenize on white space\n",
    "    return input.split()\n",
    "\n",
    "def normalize(input): # normalize as lower case\n",
    "    return (input, input.lower())\n",
    "\n",
    "sample = \"Hello, Mom!\"\n",
    "tokens = tokenize(sample)\n",
    "# print(tokens)\n",
    "normalized = [normalize(token) for token in tokens]\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NLTK word tokenization and normalize as POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tokenize(input): # use NLTK word tokenization\n",
    "    return nltk.word_tokenize(input)\n",
    "\n",
    "def normalize(input): # normalize as POS\n",
    "    pos = nltk.pos_tag([input]) # since it’s a single word, make it a list\n",
    "    print(pos)\n",
    "    return (pos)\n",
    "\n",
    "sample = \"Hello, Mom!\"\n",
    "tokens = tokenize(sample)\n",
    "# print(tokens)\n",
    "normalized = [normalize(token) for token in tokens]\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NLTK word tokenization and strip vowels and punctuation to normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(input): # use NLTK word tokenization\n",
    "    return nltk.word_tokenize(input)\n",
    "\n",
    "def normalize(input): # normalize as POS\n",
    "    return (input, re.sub('[AEIOUaeiou]','',input))\n",
    "\n",
    "sample = \"Hello, Mom!\"\n",
    "tokens = tokenize(sample)\n",
    "# print(tokens)\n",
    "normalized = [normalize(token) for token in tokens]\n",
    "print(normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}